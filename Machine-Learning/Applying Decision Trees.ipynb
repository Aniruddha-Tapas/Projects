{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Decision Trees\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem : Advertisement Banner Blocker\n",
    "Here we will discuss classification and regression with models called decision trees. We\n",
    "use an ensemble of decision trees to construct a banner advertisement blocker.\n",
    "\n",
    "We will use decision trees to create software that can block banner ads on web pages.\n",
    "This program will predict whether each of the images on a web page is an\n",
    "advertisement or article content. Images that are classified as being advertisements\n",
    "could then be hidden using Cascading Style Sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Set: \n",
    "The dataset for this example can be downloaded from http://archive.ics.uci.edu/ml/machine-learning-databases/internet_ads/ad-dataset.zip\n",
    "We will train a decision tree\n",
    "classifier using this Internet Advertisements Data Set, which contains data for 3,279 images.\n",
    "The proportions of the classes are skewed; 459 of the images are advertisements and\n",
    "2,820 are content. Decision tree learning algorithms can produce biased trees from data\n",
    "with unbalanced class proportions; we will evaluate a model on the unaltered data set\n",
    "before deciding if it is worth balancing the training data by over- or under-sampling\n",
    "instances.\n",
    "\n",
    "Information regarding the dataset can be found here:\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/internet_ads/ad.DOCUMENTATION \n",
    "\n",
    "The explanatory variables are the dimensions of the image, words from the\n",
    "containing page's URL, words from the image's URL, the image's alt text, the image's\n",
    "anchor text, and a window of words surrounding the image tag. The response variable\n",
    "is the image's class. The explanatory variables have already been transformed into\n",
    "feature representations. The first three features are real numbers that encode the width,\n",
    "height, and aspect ratio of the images. The remaining features encode binary term\n",
    "frequencies for the text variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will grid search for the\n",
    "hyperparameter values that produce the decision tree with the greatest accuracy,\n",
    "and then evaluate the tree's performance on a test set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing our imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2902: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/ad.data', header=None)\n",
    "explanatory_variable_columns = set(df.columns.values)\n",
    "response_variable_column = df[len(df.columns.values)-1]\n",
    "# The last column describes the targets\n",
    "explanatory_variable_columns.remove(len(df.columns.values)-1)\n",
    "y = [1 if e == 'ad.' else 0 for e in response_variable_column]\n",
    "X = df[list(explanatory_variable_columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encoded the advertisements as the positive class and the content as the negative\n",
    "class. More than one quarter of the instances are missing at least one of the values\n",
    "for the image's dimensions. These missing values are marked by whitespace and a\n",
    "question mark. We replaced the missing values with negative one, but we could have\n",
    "imputed the missing values; for instance, we could have replaced the missing height\n",
    "values with the average height value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda2\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "X.replace(to_replace=' *\\?', value=-1, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Splitting the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a pipeline and an instance of DecisionTreeClassifier. Then, we set\n",
    "the criterion keyword argument to entropy to build the tree using the information\n",
    "gain heuristic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "('clf', DecisionTreeClassifier(criterion='entropy'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specified the hyperparameter space for the grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "'clf__max_depth': (150, 155, 160),\n",
    "'clf__min_samples_split': (1, 2, 3),\n",
    "'clf__min_samples_leaf': (1, 2, 3)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set GridSearchCV() to maximize the model's F1 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:  5.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Best score: 0.874\n",
      "Best parameters set:\n",
      "\tclf__max_depth: 160\n",
      "\tclf__min_samples_leaf: 1\n",
      "\tclf__min_samples_split: 2\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.98       702\n",
      "          1       0.87      0.95      0.91       118\n",
      "\n",
      "avg / total       0.97      0.97      0.97       820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print 'Best score: %0.3f' % grid_search.best_score_\n",
    "print 'Best parameters set:'\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print '\\t%s: %r' % (param_name, best_parameters[param_name])\n",
    "predictions = grid_search.predict(X_test)\n",
    "print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the results show approximately 87 percent of the images that the classifier predicted were ads, were truly ads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we discussed how to train decision trees using the ID3 algorithm, which recursively splits the training instances into subsets that reduce our uncertainty about the value of the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving our model's performance using Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble learning methods combine a set of models to produce an estimator that\n",
    "has better predictive performance than its individual components. A Random Forest\n",
    "is a collection of decision trees that have been trained on randomly selected subsets\n",
    "of the training instances and explanatory variables. Random forests usually make\n",
    "predictions by returning the mode or mean of the predictions of their constituent\n",
    "trees; scikit-learn's implementations return the mean of the trees' predictions.\n",
    "\n",
    "Random forests are less prone to overfitting(Fit the training data perfectly, but do a very poor job of prediction on the test set) than decision trees because no single\n",
    "tree can learn from all of the instances and explanatory variables; no single tree can\n",
    "memorize all of the noise in the representation.\n",
    "\n",
    "Here we will update our ad blocker's classifier to use a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the DecisionTreeClassifier using scikit-learn's API by replacing the object with an instance of RandomForestClassifier. Like the previous example, we will\n",
    "# Importing the RandomForestClassifier class from the ensemble module:\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replacing the DecisionTreeClassifier in the pipeline with an instance of RandomForestClassifier and updating the hyperparameter space:\n",
    "\n",
    "pipeline = Pipeline([\n",
    "('clf', RandomForestClassifier(criterion='entropy'))\n",
    "])\n",
    "parameters = {\n",
    "'clf__n_estimators': (5, 10, 20, 50),\n",
    "'clf__max_depth': (50, 150, 250),\n",
    "'clf__min_samples_split': (1, 2, 3),\n",
    "'clf__min_samples_leaf': (1, 2, 3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grid Search to find the values of the hyperparameters that produce the random forest with the best predictive performance.\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print 'Best score: %0.3f' % grid_search.best_score_\n",
    "print 'Best parameters set:'\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "predictions = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing the single decision tree with a random forest resulted in a significant\n",
    "reduction of the error rate; the random forest improves the precision and recall for\n",
    "ads to 0.97 and 0.83."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Thus here we discussed ensemble learning methods,\n",
    "which combine the predictions from a set of models to produce an estimator with\n",
    "better predictive performance. Finally, we used random forests to improve our decision tree model's performance in predicting whether or\n",
    "not an image on a web page is a banner advertisement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Binary Classification with Logistic Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In many classification problems, there are more than two classes that are of interest. We might wish to predict the genres of songs from samples of audio, or classify images of galaxies by their types. The goal of multi-class classification is to assign an instance to one of the set of classes. scikit-learn uses a strategy called one-vs.-all, or one-vs.-the-rest, to support multi-class classification. One-vs-all classification uses one binary classifier for each of the possible classes. The class that is predicted with the greatest confidence is assigned to the instance.\n",
    "LogisticRegression supports multi-class classification using the one-versus-all strategy out of the box. Let's use LogisticRegression for a multi-class classification problem._\n",
    "\n",
    "## The Problem: \n",
    "\n",
    "Here we want to use scikit-learn to find the movies with good reviews.\n",
    "In this example, we will classify the sentiments of phrases taken from movie reviews in the Rotten Tomatoes data set. Each phrase can be classified as one of the following sentiments: negative, somewhat negative, neutral, somewhat positive, or positive. \n",
    "While the classes appear to be ordered, the explanatory variables that we will use do not always corroborate this order due to sarcasm, negation, and other linguistic phenomena. Instead, we will approach this problem as a multi-class classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Set:\n",
    "The data can be downloaded from https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data. \n",
    "This Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhraseId      156060\n",
      "SentenceId    156060\n",
      "Phrase        156060\n",
      "Sentiment     156060\n",
      "dtype: int64\n",
      "--------------------\n",
      "   PhraseId  SentenceId                                             Phrase  \\\n",
      "0         1           1  A series of escapades demonstrating the adage ...   \n",
      "1         2           1  A series of escapades demonstrating the adage ...   \n",
      "2         3           1                                           A series   \n",
      "3         4           1                                                  A   \n",
      "4         5           1                                             series   \n",
      "\n",
      "   Sentiment  \n",
      "0          1  \n",
      "1          2  \n",
      "2          2  \n",
      "3          2  \n",
      "4          2  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/train.tsv', header=0, delimiter='\\t')\n",
    "print df.count()\n",
    "print \"--------------------\"\n",
    "print df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sentiment column contains the response variables. The 0 label corresponds to the sentiment negative, 1 corresponds to somewhat negative, and so on. The Phrase column contains the raw text. Each sentence from the movie reviews has been parsed into smaller phrases. We won't require the PhraseId and SentenceId columns in this example. Let's print some of the phrases and examine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    A series of escapades demonstrating the adage ...\n",
      "1    A series of escapades demonstrating the adage ...\n",
      "2                                             A series\n",
      "3                                                    A\n",
      "4                                               series\n",
      "5    of escapades demonstrating the adage that what...\n",
      "6                                                   of\n",
      "7    escapades demonstrating the adage that what is...\n",
      "8                                            escapades\n",
      "9    demonstrating the adage that what is good for ...\n",
      "Name: Phrase, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print df['Phrase'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    156060.000000\n",
      "mean          2.063578\n",
      "std           0.893832\n",
      "min           0.000000\n",
      "25%           2.000000\n",
      "50%           2.000000\n",
      "75%           3.000000\n",
      "max           4.000000\n",
      "Name: Sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Examining the target class\n",
    "print df['Sentiment'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    79582\n",
      "3    32927\n",
      "1    27273\n",
      "4     9206\n",
      "0     7072\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    0.509945\n",
      "3    0.210989\n",
      "1    0.174760\n",
      "4    0.058990\n",
      "0    0.045316\n",
      "Name: Sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print df['Sentiment'].value_counts()/df['Sentiment'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common class, Neutral, includes more than 50 percent of the instances.\n",
    "Accuracy will not be an informative performance measure for this problem, as a\n",
    "degenerate classifier that predicts only Neutral can obtain an accuracy near 0.5.\n",
    "Approximately one quarter of the reviews are positive or somewhat positive, and\n",
    "approximately one fifth of the reviews are negative or somewhat negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifier with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda2\\lib\\site-packages\\sklearn\\metrics\\metrics.py:4: DeprecationWarning: sklearn.metrics.metrics is deprecated and will be removed in 0.18. Please import from sklearn.metrics\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=3)]: Done  72 out of  72 | elapsed:  8.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best score: 0.619\n",
      "Best parameters set:\n",
      "\tclf__C: 10\n",
      "\tvect__max_df: 0.25\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__use_idf: False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(stop_words='english')),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "parameters = {\n",
    "    'vect__max_df': (0.25, 0.5),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'vect__use_idf': (True, False),\n",
    "    'clf__C': (0.1, 1, 10),\n",
    "}\n",
    "\n",
    "df = pd.read_csv('datasets/train.tsv', header=0, delimiter='\\t')\n",
    "X, y = df['Phrase'], df['Sentiment'].as_matrix()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5)\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=3,verbose=1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print 'Best score: %0.3f' % grid_search.best_score_\n",
    "print 'Best parameters set:'\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print '\\t%s: %r' % (param_name, best_parameters[param_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our classifier using Multi-class classification performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.638164808407\n",
      "Confusion Matrix: [[ 1129  1719   608    69    11]\n",
      " [  905  6033  6210   561    16]\n",
      " [  205  3117 32703  3523   161]\n",
      " [   33   412  6506  8244  1242]\n",
      " [    6    51   525  2354  1687]]\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.32      0.39      3536\n",
      "          1       0.53      0.44      0.48     13725\n",
      "          2       0.70      0.82      0.76     39709\n",
      "          3       0.56      0.50      0.53     16437\n",
      "          4       0.54      0.36      0.44      4623\n",
      "\n",
      "avg / total       0.62      0.64      0.63     78030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = grid_search.predict(X_test)\n",
    "print 'Accuracy:', accuracy_score(y_test, predictions)\n",
    "print 'Confusion Matrix:', confusion_matrix(y_test, predictions)\n",
    "print 'Classification Report:', classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, we make predictions using the best parameter set found by using grid searching.\n",
    "While our classifier is an improvement over the baseline classifier, it frequently\n",
    "mistakes Somewhat Positive and Somewhat Negative for Neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "Here we then discussed multi-class classification, a task in which each instance must be assigned one label from a set of labels. We used the one-vs.-all strategy to classify the sentiments of movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
